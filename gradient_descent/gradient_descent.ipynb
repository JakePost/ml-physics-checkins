{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple notebook to explore Gradient Descent methods linear data with some (non-Gaussian) scatter.\n",
    "\n",
    "It accompanies Chapter 5 of the book (2 of 5).\n",
    "\n",
    "Author: Viviana Acquaviva, with contributions by Jake Postiglione and Olga Privman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16) #set seed for reproducibility purposes\n",
    "\n",
    "x = np.arange(100) \n",
    "\n",
    "yp = 3*x + 3 + 2*(np.random.poisson(3*x+3,100)-(3*x+3)) #generate some data with scatter following Poisson distribution \n",
    "                                                       #with exp value = y from linear model, centered around 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use our data set with outliers from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12) #set \n",
    "out = np.random.choice(100,15) #select 15 outliers indexes\n",
    "yp_wo = np.copy(yp)\n",
    "np.random.seed(12) #set again\n",
    "yp_wo[out] = yp_wo[out] + 5*np.random.rand(15)*yp[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,yp_wo)\n",
    "plt.scatter(x,yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect for the MSE loss right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1,1),yp_wo)\n",
    "\n",
    "slope, intercept = model.coef_, model.intercept_\n",
    "\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "What is the main difference in the slope and intercept you found, compared to the case of no outliers?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "The slope changes noticeably, from ~3 to ~4, because the outliers heavily affect the MSE loss. The intercepts also changes significantly (but remember that the intercept is much harder to pinpoint in a linear problem).\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now implement the simplest form of gradient descent: batch, stochastic, and mini-batch, one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((100, 1)), x] # add x0 = 1 to each instance; this is the bias term\n",
    "\n",
    "print(X.shape) #shape is number of instances x number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ne = np.array([[1.548],[3.978]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ne = np.mean((X.dot(theta_ne) - yp_wo.reshape(-1,1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10) #same initial conditions for all\n",
    "\n",
    "eta = 0.0001\n",
    "n_iterations = 1000 #try changing this number!\n",
    "m = 100\n",
    "\n",
    "theta_path_bgd = []\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X.T.dot(X.dot(theta) - yp_wo.reshape(-1,1))\n",
    "    theta = theta - eta * gradients\n",
    "    theta_path_bgd.append(theta)\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd) #save the path\n",
    "\n",
    "theta_bgd = theta #final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bgd = np.sum(1/m*(X.dot(theta_bgd) - yp_wo.reshape(-1,1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "\n",
    "What is the percent difference between the final value of the loss found by BGD and by the normal equation?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "(loss_ne-loss_bgd)/loss_ne*100\n",
    "\n",
    "(it should be of the order of 10^-5, showing that BGD and NE are essentially equivalent.)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "What happens to this comparison if you increase the number of iterations in BGD?? \n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "They should get closer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10) #same initial conditions for all\n",
    "\n",
    "theta = np.random.randn(2,1) \n",
    "\n",
    "eta = 0.000005\n",
    "\n",
    "n_iterations = 10000 #more iterations\n",
    "\n",
    "theta_path_sgd = []\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    \n",
    "        random_index = np.random.randint(m) # pick one example from the data \n",
    "        \n",
    "        x_one = X[random_index:random_index+1]\n",
    "        \n",
    "        y_one = yp_wo[random_index:random_index+1]\n",
    "        \n",
    "        gradients = 2 * x_one.T.dot(x_one.dot(theta) - y_one)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 \n",
    "\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "\n",
    "theta_sgd = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we find a similar theta, but not exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sgd = np.sum(1/m*(X.dot(theta_sgd) - yp_wo.reshape(-1,1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss_ne-loss_sgd)/loss_sgd*100 #percent difference with normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Should we be worried that the final value of the loss for the SGD is not that close to the one found by the Normal equation?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "No, because we know that the statistical fluctuations of the SGD algorithms are large, and the loss is not guaranteed to decrease at every step.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also implementation notes here: https://sebastianraschka.com/faq/docs/sgd-methods.html\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "theta = np.random.randn(2,1) \n",
    "\n",
    "eta = 0.000005\n",
    "\n",
    "n_iterations = 1000\n",
    "\n",
    "theta_path_mgd = []\n",
    "\n",
    "minibatch_size = 10 #size of the mini batch\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    \n",
    "    shuffled_indices = np.random.permutation(m) #shuffle array \n",
    "    \n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    \n",
    "    y_shuffled = yp_wo.reshape(-1,1)[shuffled_indices]\n",
    "    \n",
    "    xi = X_shuffled[:minibatch_size]\n",
    "    \n",
    "    yi = y_shuffled[:minibatch_size]\n",
    "    \n",
    "    gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "    \n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "    theta_path_mgd.append(theta)\n",
    "\n",
    "theta_path_mgd = np.array(theta_path_mgd)\n",
    "\n",
    "theta_mgd = theta \n",
    "\n",
    "print(theta_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mgd = np.sum(1/m*(X.dot(theta_mgd) - yp_wo.reshape(-1,1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss_ne-loss_mgd)/loss_ne*100 #percent difference with normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's most interesting to actually look at the path taken by GD in the three cases. Increasingly dark colors denote later steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "plt.scatter(theta_path_sgd[::10, 0].flatten(), theta_path_sgd[::10, 1].flatten(), marker = 's', s = 5, \\\n",
    "         label=\"Stochastic GD, N$_{it}$ = 10000\", c = np.arange(1000), cmap=plt.cm.Purples)\n",
    "plt.scatter(theta_path_mgd[:, 0].flatten(), theta_path_mgd[:, 1].flatten(), marker = \"+\", s = 12, linewidth=1, \\\n",
    "            label=\"Mini-batch GD, N$_{it}$ = 1000\", c = np.arange(1000), cmap=plt.cm.Greens)\n",
    "plt.scatter(theta_path_bgd[:, 0].flatten(), theta_path_bgd[:, 1].flatten(), marker = \"d\", s = 12, linewidth=1, \\\n",
    "            label=\"Batch GD, N$_{it}$ = 1000\", c = np.arange(1000,0,-1), cmap=plt.cm.copper)\n",
    "\n",
    "plt.scatter(theta_sgd[0],theta_sgd[1], marker = \"s\", s = 100, color = 'Purple', alpha = 0.5)\n",
    "plt.scatter(theta_mgd[0],theta_mgd[1], marker = \"+\", s = 200, color = 'DarkGreen', alpha = 1)\n",
    "plt.scatter(theta_bgd[0],theta_bgd[1], marker = \"d\", s = 100, color = 'k', alpha = 0.5)\n",
    "#plt.text(1.5,3.978,'Normal Equation solution X')\n",
    "\n",
    "legend = plt.legend(loc=\"upper left\", fontsize=16)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    legend.legendHandles[i].set_color('k')\n",
    "    legend.legendHandles[i]._sizes = [30]\n",
    "\n",
    "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\theta_1$   \", fontsize=20)\n",
    "\n",
    "plt.axis([1.3, 1.4, 2.5, 6.5])\n",
    "\n",
    "#plt.savefig('AllThePaths.png', dpi = 300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch GD\n",
    "\n",
    "np.random.seed(10) #same initial conditions for all\n",
    "\n",
    "eta = 0.0001\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta_path_bgd = []\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X.T.dot(X.dot(theta) - yp.reshape(-1,1))\n",
    "    theta = theta - eta * gradients\n",
    "    theta_path_bgd.append(theta)\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "\n",
    "#Stochastic GD:\n",
    "\n",
    "np.random.seed(10) #same initial conditions for all\n",
    "\n",
    "theta = np.random.randn(2,1) \n",
    "\n",
    "eta = 0.00005\n",
    "\n",
    "n_iterations = 1000\n",
    "\n",
    "theta_path_sgd = []\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    \n",
    "        random_index = np.random.randint(m) # pick one example from the data \n",
    "        xi = X[random_index:random_index+1]\n",
    "        yi = yp[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 # not shown\n",
    "\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "\n",
    "\n",
    "#Mini batch GD:\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "theta = np.random.randn(2,1) \n",
    "\n",
    "eta = 0.0001\n",
    "\n",
    "n_iterations = 1000\n",
    "\n",
    "theta_path_mgd = []\n",
    "\n",
    "minibatch_size = 10\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    \n",
    "    shuffled_indices = np.random.permutation(m) #shuffle array \n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = yp.reshape(-1,1)[shuffled_indices]\n",
    "    \n",
    "    xi = X_shuffled[0:minibatch_size] #without replacement, technically I should\n",
    "    yi = y_shuffled[0:minibatch_size]\n",
    "    gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "    theta = theta - eta * gradients\n",
    "    theta_path_mgd.append(theta)\n",
    "\n",
    "theta_path_mgd = np.array(theta_path_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.scatter(theta_path_bgd[:, 0].flatten(), theta_path_bgd[:, 1].flatten(), marker = \"d\", s = 12, linewidth=1, \\\n",
    "            label=\"Batch\", c = np.arange(1000,0,-1), cmap=plt.cm.copper)\n",
    "plt.scatter(theta_path_sgd[:, 0].flatten(), theta_path_sgd[:, 1].flatten(), marker = 's', s = 5, \\\n",
    "         label=\"Stochastic\", c = np.arange(1000), cmap=plt.cm.Purples)\n",
    "plt.scatter(theta_path_mgd[:, 0].flatten(), theta_path_mgd[:, 1].flatten(), marker = \"+\", s = 12, linewidth=1, \\\n",
    "            label=\"Mini-batch\", c = np.arange(1000), cmap=plt.cm.Greens)\n",
    "legend = plt.legend(loc=\"upper left\", fontsize=16)\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    legend.legendHandles[i].set_color('k')\n",
    "    legend.legendHandles[i]._sizes = [30]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: note what happens for larger learning rates and smaller learning rates. Would an adaptive learning rate be a solution? Qualitatively, how would you choose it?\n",
    "\n",
    "Exercise 2: Examine the gradients to discover why batch GD stops updating the slope pretty quickly. Would this be a concern in terms of getting stuck on local minima (in loss function that are not convex)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
