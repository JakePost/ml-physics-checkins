{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIb7AW-J_Q0j"
   },
   "source": [
    "In this simple notebook we use a fully connected neural network to solve a previously seen problem in regression: the photometric redshift problem.\n",
    "\n",
    "It accompanies Chapter 8 of the book.\n",
    "\n",
    "Author: Viviana Acquaviva, with contributions by Jake Postiglione and Olga Privman.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCi2a2GB_Q0m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty1aqrju_Q0m"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RELqQBID_Q0n"
   },
   "source": [
    "Tensorflow is a very commonly used library used in development of Deep Learning models. It is an open-source platform that was developed by Google. It supports programming in several languages, e.g. C++, Java, Python, and many others.\n",
    "\n",
    "Keras is a high-level API (Application Programming Interface) that is built on top of TensorFlow (or Theano, another Deep Learning library). It is Python-specific, and we can think of it as the equivalent of the sklearn library for neural network. It is less general, and less customizable, but it is very user-friendly and comparatively easier than TensorFlow. We will use keras with the tensorflow back-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAX7Ddfd_Q0n"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "qN9_Zvz0_fE_",
    "outputId": "54d4e0c7-cde4-4e18-b04e-821017a24210"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qQMC772_Q02"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzdIblJi_Q0n"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential #the model is built adding layers one after the other\n",
    "\n",
    "from keras.layers import Dense #fully connected layers: every output talks to every input\n",
    "\n",
    "from keras.layers import Dropout #for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPe5Uv5D_xoZ",
    "outputId": "e2e1fdff-9521-4e2e-8bd9-2c9bbbb0a032"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "# %cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLHnWtsJ_Q04"
   },
   "source": [
    "### Problem 2: photometric redshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6-Ir38f_Q05"
   },
   "source": [
    "I will start out from the reduced (high-quality) data set we used for Bagging and Boosting methods. For reference, our best model achieved a NMAD around 0.02 and an outlier fraction of 4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8hFO3qM_Q05"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/sel_features.csv', sep = '\\t')\n",
    "y = pd.read_csv('../data/sel_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "100MCfNg_Q05"
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JEkymDS_Q05"
   },
   "outputs": [],
   "source": [
    "fifth = int(len(y)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II0EN1pV_Q05"
   },
   "outputs": [],
   "source": [
    "X_train = X.values[:3*fifth,:]\n",
    "y_train = y[:3*fifth]\n",
    "\n",
    "X_val = X.values[3*fifth:4*fifth,:]\n",
    "y_val = y[3*fifth:4*fifth]\n",
    "\n",
    "X_test = X.values[4*fifth:,:]\n",
    "y_test = y[4*fifth:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aHqfJPv_Q05"
   },
   "source": [
    "We know that we need to scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGcMKwtu_Q05",
    "outputId": "5376d9d7-850c-4017-e4ee-55d09c5a61ee"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaMj1-O0_Q06"
   },
   "outputs": [],
   "source": [
    "Xst_train = scaler.transform(X_train)\n",
    "Xst_val = scaler.transform(X_val)\n",
    "Xst_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1zYQ1yg_Q06"
   },
   "source": [
    "In a regression problem, we will choose a different activation for the output layer (e.g. linear), and a different loss function (MSE, MAE, ...).\n",
    "\n",
    "Our input layer has six neurons for this problem.\n",
    "\n",
    "For other parameters and the network structure, we can start with two layers with 100 neurons and go from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EaKOc1pv_Q06",
    "outputId": "6b25fae7-ae1f-4fff-de55-458e9cedfc54"
   },
   "outputs": [],
   "source": [
    "dir(keras.activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aej9n5gY_Q06",
    "outputId": "d9b42c59-b919-4960-e33c-579fc23b835a"
   },
   "outputs": [],
   "source": [
    "dir(keras.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t37u_TiV_Q06"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Add an input layer and specify its size (number of original features)\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_shape=(6,)))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "#model.add(Dense(30, activation='relu'))\n",
    "\n",
    "# Add one hidden layer and specify its size\n",
    "\n",
    "#model.add(Dense(12, activation='relu'))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Add an output layer \n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_pO8Jjl_Q06"
   },
   "source": [
    "We begin with 100 epochs and batch size = 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpC58myL_Q06",
    "outputId": "611963d5-d486-4396-b936-2fd06b0f092b"
   },
   "outputs": [],
   "source": [
    "mynet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9F0vSIG_Q07",
    "outputId": "8d166867-9170-4f36-e794-7185cd068af4"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(Xst_test, y_test)\n",
    "print('MSE:', results) #we are only monitoring MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o1--T2H_Q07"
   },
   "source": [
    "As usual, we can plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "igJTaSwy_Q07",
    "outputId": "f7971bc6-1dcb-4143-f60f-60047954accc"
   },
   "outputs": [],
   "source": [
    "plt.plot(mynet.history['loss'], label = 'train')\n",
    "plt.plot(mynet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "plt.legend(fontsize = 12);\n",
    "#plt.savefig('Photoz_NN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4j5LEBae_Q07",
    "outputId": "cb5c6c37-030b-4996-bf80-d8b8b13e2f26"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "    \n",
    "plt.xlabel('True redshift', fontsize = 14)\n",
    "plt.ylabel('Estimated redshift', fontsize = 14)\n",
    "\n",
    "plt.scatter(y_test, model.predict(Xst_test), s =10, c = 'teal');\n",
    "\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(0,2)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Photoz_NN_scatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB8PgXsG_Q08"
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(Xst_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMxHlNxDTD-h"
   },
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Calculate the Outlier Fraction and the Normalized Median Absolute Deviation for this set of predictions.\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: line-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "print(len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test))\n",
    "\n",
    "print(1.48*np.median(np.abs(y_test-ypred)/(1 + y_test)))\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7rslnmK_Q08"
   },
   "source": [
    "To further improve, we can play with/optimize the parameters; one thing that is very interesting IMO is to see the effect of using different losses on the residuals, and trying to add more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fV8VkFC_Q08"
   },
   "source": [
    "### Let's try some optimization with keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "veJnCwk8_Q08"
   },
   "outputs": [],
   "source": [
    "# !pip3 install -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvw7d9oK_Q09",
    "outputId": "7b1ac38d-69fd-4964-af78-db4b2a74155a"
   },
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Some material below is adapted from the Keras Tuner documentation\n",
    "\n",
    "# https://keras-team.github.io/keras-tuner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXOaqAvM_Q09"
   },
   "source": [
    "This function specifies which parameters we want to tune. Tunable parameters can be of type \"Choice\" (we specify a set), Int, Boolean, or Float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XgKwB1J_Q09"
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 6)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=100,\n",
    "                                            max_value=500,\n",
    "                                            step=100),\n",
    "                               activation='relu'))\n",
    "    model.add(Dense(1, activation='linear')) #last one\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmZr4Dgh_Q09"
   },
   "source": [
    "Next, we specify how we want to explore the parameter space. The Random Search is the simplest choice, but often quite effective; alternatives are Hyperband (optimized Random Search where a larger fraction of models is trained for a smaller number of epochs, but only the most promising ones survive), or Bayesian Optimization, which attempts to build a probabilistic interpretation of the model scores (the posterior probability of obtaining score x, given the values of hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKTmAfeb_Q09"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=40, #number of combinations to try\n",
    "    executions_per_trial=3,\n",
    "    project_name='MyDrive/Photoz') #may need to delete or reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfwvvlOE_Q09"
   },
   "source": [
    "We can visualize the search space below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPJiiBOD_Q09",
    "outputId": "7e17dc01-3f10-4d13-f543-a25a40927270"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU7aZ8CL_Q0-"
   },
   "source": [
    "Finally, it's time to put our tuner to work. (This is a big job!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrqAgLjg_Q0-",
    "outputId": "d0498935-13bf-45c1-ccfc-216b6a9201ac"
   },
   "outputs": [],
   "source": [
    "tuner.search(Xst_train, y_train, #same signature as model.fit\n",
    "             epochs=100, validation_data=(Xst_val, y_val), batch_size=300, verbose = 0) \n",
    "\n",
    "#Note: setting verbosity to 0 would give no output until done - it took about ~30 mins on my laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FlfCn9m_Q0-"
   },
   "source": [
    "The \"results\\_summary(n)\" function gives us access to the n best models. It's useful to look at a few because often the differences are minimal, and a smaller model might be preferable! Note that the \"number of units\" parameter would have a value assigned to it for each layers (even if the number of layers is smaller in that particular realization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AeqNold_Q0-",
    "outputId": "590f50cf-4857-49e6-a37d-61a2fb6e7efa"
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOE3vVSJ_Q0-"
   },
   "source": [
    "The losses of the first few models are very similar, suggesting that 1. as usual, we need to do some form of cross-validation to be able to come up with a ranking, and 2. With 3-5 layers and a few hundred neurons per layer, the exact configuration doesn't matter too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn06Jq-T_Q0-"
   },
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0] #choose first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qAU0mn1_Q0-",
    "outputId": "31ffc5b5-4cee-41c9-9428-7077bdfb0ae3"
   },
   "outputs": [],
   "source": [
    "best_hps.get('learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cLPW3MH_Q0-",
    "outputId": "c14aa9a2-4501-4791-c891-f69d0dc1b89e"
   },
   "outputs": [],
   "source": [
    "best_hps.get('num_layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b_LsMsr_Q0-",
    "outputId": "933e16ca-232b-44ae-c7c9-5592954af517"
   },
   "outputs": [],
   "source": [
    "#Size of layers\n",
    "\n",
    "print(best_hps.get('units_0'))\n",
    "print(best_hps.get('units_1'))\n",
    "print(best_hps.get('units_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq8ULBCb_Q0_"
   },
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps) #get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vilFKjD4_Q0_"
   },
   "outputs": [],
   "source": [
    "model.build(input_shape=(None,6)) #build best model (if not fit yet, this will give access to summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nbegil5_Q0_",
    "outputId": "c2313aca-3be0-4395-ee5c-8c7ce89af241"
   },
   "outputs": [],
   "source": [
    "model.summary() #Note that this differs from what was shown in the tuner search summary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_gLSfuK_Q0_",
    "outputId": "d91368d2-60f9-4b41-ab3d-c3148bf92ce8"
   },
   "outputs": [],
   "source": [
    "bestnet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "ezQT_8iw_Q0_",
    "outputId": "a7c21133-54d9-4786-9457-e0aaa3167abe"
   },
   "outputs": [],
   "source": [
    "plt.plot(bestnet.history['loss'], label = 'train')\n",
    "plt.plot(bestnet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.ylim(0,0.1)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "plt.legend(fontsize = 12);\n",
    "#plt.savefig('OptimalNN_Photoz.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_C9_ECTO_Q0_",
    "outputId": "2729fe65-86a4-4002-f8af-eb0857fabad1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(Xst_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbvdcdQs_Q1A",
    "outputId": "c0c34493-28df-4dff-d8d4-932730c633e1"
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(Xst_test)\n",
    "\n",
    "#Calculate OLF\n",
    "\n",
    "print('OLF', len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test))\n",
    "\n",
    "#Calculate Normalized Median Absolute Deviation (NMAD)\n",
    "\n",
    "print('NMAD', 1.48*np.median(np.abs(y_test-ypred)/(1 + y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKO11my0_Q1A"
   },
   "source": [
    "These numbers have improved, compared to the baseline version - whether or not the improvement is significant should be determined via cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cAnGJLzv_Q1A",
    "outputId": "64ea0243-b088-472e-f994-64a57bb4ce54"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "    \n",
    "plt.xlabel('True redshift', fontsize = 14)\n",
    "plt.ylabel('Estimated redshift', fontsize = 14)\n",
    "\n",
    "plt.scatter(y_test, model.predict(Xst_test), s =10, c = 'teal');\n",
    "\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(0,2)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('OptimalNN_scatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fdh8_yk_Q1B"
   },
   "source": [
    "Given the gap between train and validation scores above, it might be tempting to add some regularization (this should however be included in the tuner!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQ8QwqpO_Q1B"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.add(Dense(400, activation='relu', input_shape=(6,)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwUmUzuN_Q1C",
    "outputId": "c8440787-26be-4ea7-edcc-4d432a25e7c2"
   },
   "outputs": [],
   "source": [
    "bestregnet = model.fit(Xst_train, y_train, validation_data= (Xst_val, y_val), epochs=100, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZwaiHti_Q1C",
    "outputId": "0fd72def-ee36-463c-9c52-50314318b261"
   },
   "outputs": [],
   "source": [
    "model.evaluate(Xst_test, y_test)\n",
    "\n",
    "ypred = model.predict(Xst_test)\n",
    "\n",
    "#Calculate OLF\n",
    "\n",
    "print('OLF', len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test))\n",
    "\n",
    "#Calculate Normalized Median Absolute Deviation (NMAD)\n",
    "\n",
    "print('NMAD', 1.48*np.median(np.abs(y_test-ypred)/(1 + y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "f9HllFQl_Q1D",
    "outputId": "5269efc8-062a-435d-bf8c-759181d802d7"
   },
   "outputs": [],
   "source": [
    "plt.plot(bestregnet.history['loss'], label = 'train')\n",
    "plt.plot(bestregnet.history['val_loss'],'-.m', label = 'validation')\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.ylim(0,0.1)\n",
    "plt.legend(loc='upper right', fontsize = 12)\n",
    "plt.legend(fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3avc6jx_Q1D"
   },
   "source": [
    "Note the overall effect is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV1hJxvg_Q1D"
   },
   "source": [
    "## Effect of different loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2o9BjB4_Q1D",
    "outputId": "a1c53864-565a-4d4e-e84e-322d86aa57f5"
   },
   "outputs": [],
   "source": [
    "dir(keras.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3doWzcX_Q1D"
   },
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y, random_state = 10)\n",
    "\n",
    "X_train = X.values[:3*fifth,:]\n",
    "y_train = y[:3*fifth]\n",
    "\n",
    "X_val = X.values[3*fifth:4*fifth,:]\n",
    "y_val = y[3*fifth:4*fifth]\n",
    "\n",
    "X_test = X.values[4*fifth:,:]\n",
    "y_test = y[4*fifth:]\n",
    "\n",
    "scaler.fit(X_train) #Important: we use only training data to scale\n",
    "\n",
    "Xst_train = scaler.transform(X_train)\n",
    "Xst_val = scaler.transform(X_val)\n",
    "Xst_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZNQvaAR_Q1D"
   },
   "source": [
    "### Effect of num trials; is the difference between OLF/NMAD significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJytTFAS_Q1D",
    "outputId": "c9691570-e963-49c4-e014-69d1d4bd7c31"
   },
   "outputs": [],
   "source": [
    "#Architecture stays the same\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Dense(units=500,\n",
    "                               activation='relu'))\n",
    "model.add(layers.Dense(units=100,\n",
    "                               activation='relu'))\n",
    "model.add(layers.Dense(units=400,\n",
    "                               activation='relu'))\n",
    "model.add(Dense(1, activation='linear')) #last one\n",
    "\n",
    "#We use three different loss functions and repeat the training 4x\n",
    "\n",
    "for loss in ['mse','mae', 'mape']:\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "        loss=loss)\n",
    "\n",
    "    OLF = np.zeros(4)\n",
    "    NMAD = np.zeros(4)\n",
    "\n",
    "    for i in range(0,3): #let's do this 4 times and change only random weights initialization\n",
    "    \n",
    "        model.fit(Xst_train, y_train,\n",
    "             epochs=100,\n",
    "             validation_data=(Xst_val, y_val), batch_size=300, verbose = 0)\n",
    "\n",
    "        ypred = model.predict(Xst_test)\n",
    "\n",
    "        #Calculate OLF\n",
    "\n",
    "        OLF[i] = len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test)\n",
    "\n",
    "        #Calculate Normalized Median Absolute Deviation (NMAD)\n",
    "        \n",
    "        NMAD[i] = 1.48*np.median(np.abs(y_test-ypred)/(1 + y_test))\n",
    "\n",
    "    print('OLF mean/std using loss', loss, 'is:', \"{:.3f}\".format(OLF.mean()), \"{:.3f}\".format(OLF.std()))\n",
    "    print('NMAD mean/std using loss', loss, 'is:', \"{:.2f}\".format(NMAD.mean()), \"{:.3f}\".format(NMAD.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Which loss functions are most suited to miniziming the OLF and NMAD?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "If we want to minimize OLF/NMAD, our preferred choice(s) should be the MAE or MSE losses. In alternative, we can define a custom loss. \n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2_34-jR_Q1D"
   },
   "source": [
    "### Single model evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14VFh_1D_Q1D"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0LnAnrS_Q1E"
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elPw87o6_Q1E",
    "outputId": "7b16e0ce-7ae6-4161-df9d-97b5ad2ace0e"
   },
   "outputs": [],
   "source": [
    "def create_single_model():\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(300, activation = 'relu'))\n",
    "    model.add(layers.Dense(300, activation = 'relu'))\n",
    "    model.add(layers.Dense(300, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'linear')) \n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "estimator = KerasRegressor(build_fn = create_single_model, epochs = 100, batch_size = 200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZghEBfi_Q1E"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('scale', StandardScaler()), ('model', estimator)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sL0f_ES3_Q1E"
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(pipeline, X, y, cv = cv, scoring = 'neg_mean_squared_error', return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVxXHBal_Q1E",
    "outputId": "adeb94a7-4886-415a-b186-44694b3e0c40"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u-7hbhR_Q1E",
    "outputId": "a8ca89d0-e64b-4168-d964-7d2a30263192"
   },
   "outputs": [],
   "source": [
    "scores['test_score'].mean(), scores['test_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE53He5e_Q1E"
   },
   "source": [
    "### Model optimization and scoring with cross validation/nested cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vihd6D3y_Q1E"
   },
   "source": [
    "First, we define a new function to build a keras model, and make sure we can vary the arguments we are interested in optimizing. In this case, we are keeping the number of hidden layers at 3, and varying their sizes, as well as the learning rate. Some other parameters will be added directly as part of the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3zUroNi_Q1F"
   },
   "outputs": [],
   "source": [
    "def create_model(lr = 0.01, size_1 = 500, size_2 = 100, size_3 = 400):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(units = size_1,\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(units = size_2,\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(units = size_3,\n",
    "                               activation='relu'))\n",
    "    model.add(Dense(1, activation='linear')) #last one\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
    "        loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atmCwoZv_Q1F"
   },
   "source": [
    "Next, we define our hyperparameter grid, which will be the input for our Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuG2GS5J_Q1F"
   },
   "outputs": [],
   "source": [
    "# Random search parameters\n",
    "\n",
    "batch_size = [200, 300, 400]\n",
    "\n",
    "lrs = [0.0001, 0.001, 0.01]\n",
    "\n",
    "epochs = [50, 100, 200]\n",
    "\n",
    "size_1 = [100, 300, 500]\n",
    "\n",
    "size_2 = [100, 300, 500]\n",
    "\n",
    "size_3 = [100, 300, 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YK0Vg_Np_Q1F"
   },
   "source": [
    "Then, we run a cross-validated search for the best parameters. We choose 40 model evaluations. Note that running this search takes a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADxDeh8R_Q1F",
    "outputId": "5fa51610-fc07-4541-9c51-a168cac7e81e"
   },
   "outputs": [],
   "source": [
    "kmodel = KerasRegressor(build_fn = create_model, verbose=0)\n",
    "\n",
    "pipeline = Pipeline([('scale', StandardScaler()), ('est', kmodel)])\n",
    "\n",
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0IQ37-j_Q1F"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64') #this is here because of a warning\n",
    "\n",
    "#Define cv strategy\n",
    "\n",
    "cv = KFold(n_splits = 4, shuffle = True)\n",
    "\n",
    "param_grid = dict(est__size_1 = size_1, est__size_2 = size_2, est__size_3 = size_3,\n",
    "                  est__batch_size = batch_size, est__epochs = epochs, est__lr = lrs)\n",
    "                  \n",
    "grid = RandomizedSearchCV(estimator = pipeline, param_distributions = param_grid, n_iter = 40, n_jobs=-1, \\\n",
    "                          cv=cv, return_train_score = True)\n",
    "\n",
    "results = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sUy-uuq_Q1F"
   },
   "source": [
    "We can take a look at the distribution of validation scores (it says test here, but we should really think of them as  validation) by looking at the first lines of the \"results\" object, sorted by validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "N04tcdod_Q1F",
    "outputId": "884b109c-5ac8-4d5d-9773-a5f22459ea55"
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(results.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n",
    "scoresCV.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "td3lUxgp_Q1F"
   },
   "source": [
    "This procedure settles the question of which model(s) perform(s) best, but it still doesn't provide a proper estimate of the generalization error, which should be computed on data that have never participated in the yperparameter optimization nor training process. Assessing the test scores (and their uncertainty due to the stochastic nature of sample selection and the non-deterministic aspects of the neural network) requires a three-tiered structure, with two nested CV processes: the outer CV \"peels out\" the test folds, the inner CV does the validation/parameter optimization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3llkLVp_Q1G"
   },
   "source": [
    "### Nested cross validation in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xgfwj3Mh_Q1G"
   },
   "outputs": [],
   "source": [
    "Xa = X.values #turn them into numpy array\n",
    "ya = y.values.ravel() #turn them into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBqgq3V0_Q1G"
   },
   "outputs": [],
   "source": [
    "Xa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME8ANCyN_Q1G"
   },
   "outputs": [],
   "source": [
    "#Outer and inner k-fold:\n",
    "    \n",
    "outercv = KFold(n_splits=4, shuffle=True) #creates 4 disjoint splits\n",
    "\n",
    "innercv = KFold(n_splits=3, shuffle=True) #creates 3 disjoint splits\n",
    "\n",
    "i = 0\n",
    "\n",
    "winning_model_test_scores = []\n",
    "\n",
    "OLF = []\n",
    "\n",
    "NMAD = []\n",
    "\n",
    "for train_index, test_index in outercv.split(Xa,ya): #This runs the outer cross validation\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    print('Fold ' ,i, 'outer cross validation')\n",
    "    \n",
    "    X_train = Xa[train_index] #learning set, will be used for training + validation\n",
    "    y_train = ya[train_index] \n",
    "    \n",
    "    X_test = Xa[test_index] #test set, won't know anything about training or validation\n",
    "    y_test = ya[test_index]\n",
    "    \n",
    "    #Let's scale here (and not again within the CV; this is slightly not rigorous but ok for practical purposes)\n",
    "    \n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    Xst_train = scaler.transform(X_train)\n",
    "    Xst_test = scaler.transform(X_test)\n",
    "    \n",
    "    #defining parameter grid and model\n",
    "    \n",
    "    model = KerasRegressor(build_fn = create_model, verbose=0)\n",
    "\n",
    "    # Random search parameters\n",
    "\n",
    "    batch_size = [200, 300, 400]\n",
    "\n",
    "    lrs = [0.0001, 0.001, 0.01]\n",
    "\n",
    "    epochs = [50, 100, 200]\n",
    "\n",
    "    size_1 = [100, 300, 500]\n",
    "\n",
    "    size_2 = [100, 300, 500]\n",
    "\n",
    "    size_3 = [100, 300, 500]\n",
    "\n",
    "    param_grid = dict(size_1 = size_1, size_2 = size_2, size_3 = size_3, lr = lrs, \\\n",
    "                  batch_size = batch_size, epochs = epochs)\n",
    "\n",
    "    grid = RandomizedSearchCV(estimator = model, param_distributions = param_grid, n_iter = 40, n_jobs=-1, \\\n",
    "                          cv=innercv, return_train_score = True)\n",
    "\n",
    "    results = grid.fit(Xst_train, y_train) #if you want to explore the validation search results, you should save this object\n",
    "\n",
    "    #Get best estimator; compute test scores with optimal parameters on outer i-th test fold\n",
    "    \n",
    "    winner = results.best_estimator_\n",
    "    \n",
    "    print('The winning model has parameters', results.best_params_) #This is just to compare the best model in different folds\n",
    "    \n",
    "    winner.fit(Xst_train, y_train) #we can use the entire inner learning set to train the winning model\n",
    "    \n",
    "    ypred = winner.predict(Xst_test) #X_test is totally new to the training/optimization process\n",
    "    \n",
    "    #calculate OLF and NMAD!\n",
    "    \n",
    "    OLF.append(len(np.where(np.abs(y_test-ypred)>0.15*(1+y_test))[0])/len(y_test))\n",
    "\n",
    "    #Calculate Normalized Median Absolute Deviation (NMAD)\n",
    "        \n",
    "    NMAD.append(1.48*np.median(np.abs(y_test-ypred)/(1 + y_test)))\n",
    "    \n",
    "    #Finally, save test scores\n",
    "    \n",
    "    winning_model_test_scores.append(metrics.mean_squared_error(y_test,ypred)) #append this to the outer cv results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEfe9vIt_Q1G"
   },
   "outputs": [],
   "source": [
    "print('The average MSE of the winning model (i.e. the generalization error) is', \\\n",
    "      \"{:.3f}\".format(np.mean(winning_model_test_scores)), 'with a std of', \"{:.3f}\".format(np.std(winning_model_test_scores)))\n",
    "\n",
    "print('The average OLF of the winning model is', \\\n",
    "      \"{:.3f}\".format(np.mean(OLF)), 'with a std of', \"{:.3f}\".format(np.std(OLF)))\n",
    "\n",
    "print('The average NMAD of the winning model is', \\\n",
    "      \"{:.3f}\".format(np.mean(NMAD)), 'with a std of', \"{:.3f}\".format(np.std(NMAD)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oGeHV-4_Q1G"
   },
   "source": [
    "### Notes:\n",
    "\n",
    "There are lots of random processes in training NNs -> even with one fold, because the weight initialization is random (num_trials = 3 is minimum recommended).\n",
    "\n",
    "k fold cross validation (or better, nested cross validation) should be used to find optimal model and estimate test scores.\n",
    "\n",
    "Things get expensive really fast! Random Search helps with this.\n",
    "\n",
    "Recognizing secondary parameters also helps. \n",
    "\n",
    "My recommendation is that often multiple configurations give similar results, and having single-point estimates can trick us into thinking that small differences matter more than they do. So definitely invest resources in figuring out whether a more expensive network (wider, deeper) really matters for your task. \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FirstNN_Part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
