{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple notebook to use Logistic Regression model for the Ising model.\n",
    "\n",
    "It accompanies Chapter 5 of the book (4 of 5).\n",
    "\n",
    "Author: Viviana Acquaviva, with contributions by Jake Postiglione and Olga Privman; see also data credits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate, train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's take a look at those sigmoids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 2*x + 5 #Linear bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that the probability that something will happen is called $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic model assumes that\n",
    "\n",
    "$log (\\frac{\\pi}{1-\\pi}$) = z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now solve for $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, pi)\n",
    "\n",
    "plt.xlim(-7,3);\n",
    "\n",
    "plt.title('Hello, I am a sigmoid!')\n",
    "\n",
    "plt.xlabel('x', fontsize=14)\n",
    "\n",
    "plt.ylabel('$\\pi$',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Where is that $\\pi$ = 0.5? \n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "Looking at the definition of the logistic model, we can see that $\\pi$ = 0.5 (odds are the same) when z = 0; in our graph, this corresponds to x = -2.5.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "What happens if the slope of the linear model is negative?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "The asymptotes of the sigmoid are flipped, and the curve is monotonically decreasing.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now see an example from Mehta et al 2018:\n",
    "\n",
    "[\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to use a logistic regression model to predict whether a material is in a ordered or disordered phase, based on its spin configuration. In an ordered phase, the spins are aligned. The representation is a 2D lattice so our features are the spin states of each element in the lattice. The physical model, known as Ising model, predicts that the transition depends on temperature and is smeared (for a finite-size lattice), around a critical temperature $T_c$.\n",
    "\n",
    "The training data is composed of 160,000 Monte Carlo simulations in a range of temperatures, and their labels.\n",
    "\n",
    "Possible applications of this formalism involve predicting the critical temperature for more complex systems.\n",
    "\n",
    "Reading in the data might take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is gratefully borrowed with permission from the notebooks maintained by P. Mehta.\n",
    "\n",
    "######### LOAD DATA\n",
    "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
    "data_file_name = '../data/Ising2DFM_reSample_L40_T=All.pkl'\n",
    "# The labels are obtained from the following file:\n",
    "label_file_name = '../data/Ising2DFM_reSample_L40_T=All_labels.pkl'\n",
    "\n",
    "\n",
    "#DATA\n",
    "with open(data_file_name, 'rb') as pickle_file:\n",
    "    data = pickle.load(pickle_file) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
    "with open(label_file_name, 'rb') as pickle_file:\n",
    "    labels = pickle.load(pickle_file) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(data.shape[0]),labels)\n",
    "\n",
    "plt.xlabel('Example #')\n",
    "\n",
    "plt.ylabel('State');\n",
    "\n",
    "#labels: 1 = ordered or near-critical\n",
    "#labels: 0 = disordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Is this a balanced or imbalanced data set?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "To check for balance, we can count the percentage of \"1\" labels, e.g. doing np.sum(labels)/len(labels), and we obtain ~ 56%, indicating that the data set is balanced.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can take a look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H/T: https://stackoverflow.com/questions/16834861/create-own-colormap-using-matplotlib-and-plot-color-scale\n",
    "\n",
    "cmap = matplotlib.colors.ListedColormap([\"aquamarine\",\"navy\"], name='from_list', N=None)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "axarr[0].imshow(data[0].reshape(40,40), cmap = cmap) #first object has label \"1\"\n",
    "axarr[1].imshow(data[80000].reshape(40,40), cmap = cmap) #from documentation, this is critical-ish (between 60, and 90,000)\n",
    "axarr[2].imshow(data[100000].reshape(40,40), cmap = cmap) #disordered\n",
    "for i in range(3):\n",
    "    axarr[i].set_xticks([0,20,40]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pick a random selection to speed up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "sel = np.random.choice(data.shape[0], 16000, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seldata = data[sel,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellabels = labels[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(seldata.shape[0]),sellabels); #The random selection also has the advantage of reshuffling the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now time for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 1000) #This uses a numerical method to find the minimum of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params() #Note that (unlike in linear regression) regularization is the norm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use cross validation, as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes 5-10 seconds\n",
    "\n",
    "results = cross_validate(model, seldata, sellabels, \n",
    "                         cv = KFold(n_splits=5, shuffle=True, random_state=10), return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Which metric do you think those numbers represent?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "Quite surprisingly, the standard output of Logistic Regression is accuracy (a classification metric!)\n",
    "```\n",
    "\n",
    "</p>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavior is sub-optimal because we also want to access the odds. We'll look at that in a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do our own grid search to optimize the regularization parameter C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in np.logspace(-3,3,7):\n",
    "    model = LogisticRegression(max_iter=1000, C = C)\n",
    "    results = cross_validate(model, seldata, sellabels, \n",
    "                         cv = KFold(n_splits=5, shuffle=True, random_state=10), return_train_score = True)\n",
    "    print('C/Average test accuracy for C = ', '{:.3e} {:s} {:.3f} {:s} {:.3f}'.format(C, 'is ', results['test_score'].mean(),'+-',results['test_score'].std()))\n",
    "    print('C/Average train accuracy for C = ', '{:.3e} {:s} {:.3f} {:s} {:.3f}'.format(C, 'is ', results['train_score'].mean(),'+-',results['train_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "- How is this model's performance?\n",
    "\n",
    "\n",
    "### Learning Check-in\n",
    "    \n",
    "Which value of C should we pick?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "The (test) scores are pretty flat, so the value of C that we pick is not that important.\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "How is this model's performance?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "Not great, with an accuracy around 66%.\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we generate labels in order to check predictions.\n",
    "\n",
    "For those classifiers that are solving a regression problem under the hood, there is the handy \"predict_proba\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1.0, max_iter=1000)\n",
    "\n",
    "ypred = cross_val_predict(model, seldata, sellabels, \\\n",
    "                               cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "\n",
    "ypred_prob = cross_val_predict(model, seldata, sellabels, \\\n",
    "                               cv = KFold(n_splits=5, shuffle=True, random_state=10), method = 'predict_proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of predict_proba gives the probability to belong to disordered (label 0) or ordered (label 1) phase. The simple classifier output is the class with p > 0.5. We can look at this to convince ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.column_stack([ypred_prob, ypred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can plot a few examples to see how our classifier is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(nrows=1, ncols=8, figsize=(15,5))\n",
    "for i in range(8):\n",
    "    axarr[i].imshow(seldata[i].reshape(40,40), cmap = cmap) \n",
    "    axarr[i].set_xlabel('True label:'+str(sellabels[i])+'\\n'+'Pred label:'+str(ypred[i]))\n",
    "    axarr[i].set_yticks([])\n",
    "    axarr[i].set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are two instances that are misclassified by our Log Reg classifier. However, at least visually, it is understandable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a look at the corresponding probabilities reveals some concerns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_prob[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 Ordered  (decent confidence) \n",
    "\n",
    "1 Ordered  (decent confidence) \n",
    "\n",
    "2 is predicted to be Ordered WITH HIGH CONFIDENCE... BUT INCORRECTLY!\n",
    "\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is going wrong here, because the confidence level of very uncertain cases appears to be too high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion is that the main indicator for this model is lack of consistency between spin alignments, which is not modeled well by our regressor. It's a tricky problem because many algorithms tend to look at the value of each feature to decide - for many of them, it's hard to represent the correlation among features as an indicator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Which algorithm from the ones we have seen so far would you recommend using instead of Logistic Regression?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "Something that seems important here is to be able to combine features together. This is something that (generalized) linear models can't do well, but is well within reach for Support Vector Machines, for example\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
