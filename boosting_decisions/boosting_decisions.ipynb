{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Decisions\n",
    "\n",
    "In this notebook, we build some understanding about the boosting process using Adaptive Boosting and Gradient Boosted Trees methods.\n",
    "\n",
    "The final application will be the photometric redshift problem introduced in Chapter 6; however, specific solution for those are explored in the notebook \"Flavors of Boosting\".\n",
    "\n",
    "Author: Viviana Acquaviva, with contributions by Jake Postiglione and Olga Privman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "#matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for comparison of weak learners as base estimators:\n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-3-642-20042-7_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation from scratch (with sample weights - should check if it's an original source)\n",
    "\n",
    "https://xavierbourretsicotte.github.io/AdaBoost.html\n",
    "\n",
    "or this, but I think it's inspired by the above:\n",
    "\n",
    "https://geoffruddock.com/adaboost-from-scratch-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can read the photometric redshifts data set with the selections applied in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = pd.read_csv('../data/sel_features.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_target = pd.read_csv('../data/sel_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_target.values.ravel() #changes shape to 1d row-like array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can try our usual benchmarking with AdaBoost, using default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(sel_target,ypred, s =10)\n",
    "plt.ylim(0,3)\n",
    "plt.xlim(0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where I started wondering whether the boosting process was working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note (from sklearn docs): If None, then the base estimator is DecisionTreeRegressor(max_depth=3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I decided to investigate the role of different parameters in the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing max depth in base estimators: I tried with trees making a maximum of 3, 6, and 10 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "for i, depth in enumerate([3,6,10]):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=depth))\n",
    "    ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "    plt.scatter(sel_target,ypred, s =10, c = 'teal')\n",
    "    plt.title('Max depth = '+str(depth))\n",
    "    plt.xlabel('True redshift')\n",
    "    if i == 0:\n",
    "        plt.ylabel('Estimated redshift')\n",
    "    plt.ylim(0,2)\n",
    "    plt.xlim(0,2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "#plt.savefig('AdaB_z.png')\n",
    "#    plt.axes('equal')\n",
    "#    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing N of base estimators (stages participating in boosting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(0,2)\n",
    "\n",
    "for nest in [5,10,20]:\n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=6), n_estimators=nest)\n",
    "    ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "    plt.scatter(sel_target,ypred, s =10, label = 'N est = '+str(nest))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(0,2)\n",
    "\n",
    "for loss in ['linear','square']:\n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=6), loss = loss)\n",
    "    ypred = cross_val_predict(model, sel_features,sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=10))\n",
    "    plt.scatter(sel_target,ypred, s =10, label = 'Loss = '+loss)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The conclusion of this process was that for AdaBoost at least, the base estimator needs to be \"strong enough\" in order for the boosting process to succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple regression toy model\n",
    "### Inspired by\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is what happens if max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "weakl = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# Fit regression model, saving each \"stage\"\n",
    "\n",
    "regr_1 = weakl\n",
    "\"\"\n",
    "regr_2 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=2, random_state=rng)\n",
    "\n",
    "regr_3 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=3, random_state=rng)\n",
    "\n",
    "regr_4 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=4, random_state=rng)\n",
    "\n",
    "regr_10 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=10, random_state=rng)\n",
    "\n",
    "regr_100 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "regr_4.fit(X, y)\n",
    "regr_10.fit(X, y)\n",
    "regr_100.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3 = regr_3.predict(X)\n",
    "y_4 = regr_4.predict(X)\n",
    "y_10 = regr_10.predict(X)\n",
    "\n",
    "for yp in [y_1,y_2,y_3,y_4,y_10]:\n",
    "    print('r2 score: ', np.round(metrics.r2_score(yp,y),3))\n",
    "\n",
    "# Plot the results\n",
    "\n",
    "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
    "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
    "plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
    "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
    "#plt.plot(X, y_4, \":m\", label=\"n_estimators=4\", linewidth=1)\n",
    "#plt.plot(X, y_10, \"-k\", label=\"n_estimators=10\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"AdaBoost Regression, max depth = 3\", fontsize = 14)\n",
    "plt.legend(fontsize=10);\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"AdaBoost_3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is what happens if max_depth = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "weakl = DecisionTreeRegressor(max_depth=6)\n",
    "\n",
    "# Fit regression model, saving each \"stage\"\n",
    "regr_1 = weakl\n",
    "\"\"\n",
    "regr_2 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=2, random_state=rng)\n",
    "\n",
    "regr_3 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=3, random_state=rng)\n",
    "\n",
    "regr_4 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=4, random_state=rng)\n",
    "\n",
    "regr_10 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=10, random_state=rng)\n",
    "\n",
    "regr_100 = AdaBoostRegressor(weakl,\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "regr_4.fit(X, y)\n",
    "regr_10.fit(X, y)\n",
    "regr_100.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3 = regr_3.predict(X)\n",
    "y_4 = regr_4.predict(X)\n",
    "y_10 = regr_10.predict(X)\n",
    "\n",
    "for yp in [y_1,y_2,y_3,y_4,y_10]:\n",
    "    print(metrics.r2_score(yp,y))\n",
    "\n",
    "# Plot the results\n",
    "\n",
    "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
    "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
    "plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
    "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
    "#plt.plot(X, y_4, \":m\", label=\"n_estimators=4\", linewidth=1)\n",
    "#plt.plot(X, y_10, \"-k\", label=\"n_estimators=10\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"AdaBoost Regression, max depth = 6\", fontsize = 14)\n",
    "plt.legend(fontsize=10);\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"AdaBoost_6.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Based on the figure above, is the boosting process worth it for AdaBoost with a base learner tree with max depth = 3? How about one with max depth = 6?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "It is not worth it for the first case (max depth = 3), as we see that the r2 scores don't improve if we stack more estimators. It may be worth for max depth = 6, but the scores are essentially stable, so further investigation may be needed.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we are convinced, let's go back to photo-zs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a train/test split because I need to access the \"staged_predict\" property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin with very weak learner (r2 = 0.4)\n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),\n",
    "                  n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the R2 score and the Spearman correlation coefficient between true and predicted values as a function of the number of stages/iterations, beginning with a weak base learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 30\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.ylim(0,1.0)\n",
    "\n",
    "plt.title('Max depth = 3')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scores don't seem to improve as we stack more estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try again with a stronger base learner (max_depth = 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 30\n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=6),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 6')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an even stronger base learner (max_depth = 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 30\n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),\n",
    "                  n_estimators=30)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 10')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's combine all in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "n_estimators = 30\n",
    "\n",
    "for i, md in enumerate([3,6,10]):\n",
    "    \n",
    "    model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=md),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
    "\n",
    "    plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r', c = 'fuchsia')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.ylim(0,1.0)\n",
    "\n",
    "    plt.title('Max depth = '+str(md)+', AdaBoost')\n",
    "    \n",
    "    if i == 2:\n",
    "        plt.legend();\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('AdaB_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Based on the figure above, would you recommend using AdaBoost with a base learner with max depth = 6, and 30 iterations, or with max depth = 10, and 10 iterations?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "The R2 scores and the correlation between true and predicted values are both higher for the case of max depth = 6 and 10 iterations, so that would be the correct choice.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort-of have an answer from the third panel of the figure above, but we could also ask whether we should keep boosting (i.e. if adding more stages is beneficial.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shall we keep boosting? (max_depth = 10)\n",
    "\n",
    "n_estimators = 60\n",
    "\n",
    "model= AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(sel_features,sel_target.values.ravel(), test_size=.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2')\n",
    "\n",
    "plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.title('Base estimator, max depth = 10')\n",
    "\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: stacking learners that are too weak doesn't help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Would this be true also for Gradient Boosted Trees algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one way to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters depend on the particular implementation.\n",
    "\n",
    "In the sklearn formulation, the parameters of each tree are essentially the same we have for Random Forests; additionally we have the \"learning_rate\" parameter, which dictates how much each tree contribute to the final estimator, and the \"subsample\" parameters, which allows one to use a < 1.0 fraction of samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how this works with a weak learner on our toy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 4, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "weakl = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = weakl\n",
    "\"\"\n",
    "regr_2 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=2, random_state=rng)\n",
    "\n",
    "regr_3 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=3, random_state=rng)\n",
    "\n",
    "regr_4 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=4, random_state=rng)\n",
    "\n",
    "regr_10 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=10, random_state=rng)\n",
    "\n",
    "regr_100 = GradientBoostingRegressor(max_depth=3,\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "regr_3.fit(X, y)\n",
    "regr_4.fit(X, y)\n",
    "regr_10.fit(X, y)\n",
    "regr_100.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "y_3 = regr_3.predict(X)\n",
    "y_4 = regr_4.predict(X)\n",
    "y_10 = regr_10.predict(X)\n",
    "y_100 = regr_100.predict(X)\n",
    "\n",
    "for yp in [y_1,y_2,y_3,y_4,y_10, y_100]:\n",
    "    print('R2 score: ', np.round(metrics.r2_score(yp,y),3))\n",
    "\n",
    "# Plot the results\n",
    "\n",
    "plt.scatter(X, y, c=\"k\", s=10,label=\"training samples\")\n",
    "plt.plot(X, y_1, \"-g\", label=\"n_estimators=1\", linewidth=1)\n",
    "#plt.plot(X, y_2, \"--r\", label=\"n_estimators=2\", linewidth=1)\n",
    "plt.plot(X, y_3, \"-.b\", label=\"n_estimators=3\", linewidth=1)\n",
    "#plt.plot(X, y_4, \":m\", label=\"n_estimators=4\", linewidth=1)\n",
    "plt.plot(X, y_10, \"-k\", label=\"n_estimators=10\", linewidth=1)\n",
    "plt.plot(X, y_100, \"-c\", label=\"n_estimators=100\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.ylim(-2.5,2.5)\n",
    "plt.title(\"Gradient Boosting Regression, max depth = 3\", fontsize = 14)\n",
    "plt.legend(fontsize=14, loc = 'upper right');\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"GradBoost_3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Some of the R2 scores seen above for GBTs are negative! Shouldn't the R2 scores always be positive?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "No, the requirement only holds for the training set. A negative R2 score on the test set (or on the validation set) simply indicates that the model performs worse than a constant prediction equal to the mean of the sample. So that would be a terrible model, but (possibly ;) ) not a coding mistake.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "n_estimators = 30\n",
    "\n",
    "for i, md in enumerate([3,6,10]):\n",
    "    \n",
    "    model = GradientBoostingRegressor(max_depth=md,\n",
    "                  n_estimators=n_estimators)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    plt.plot(range(n_estimators), [metrics.r2_score(y_test,list(model.staged_predict(X_test))[i]) for i in range(n_estimators)], label = 'r2 score', c = 'steelblue')\n",
    "\n",
    "    plt.plot(range(n_estimators), [stats.spearmanr(y_test,list(model.staged_predict(X_test))[i])[0] for i in range(n_estimators)], label = 'Spearman r', c = 'fuchsia')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "    plt.ylim(0,1.0)\n",
    "\n",
    "    plt.title('Max depth = '+str(md)+', GBR')\n",
    "    \n",
    "    if i == 2:\n",
    "        plt.legend();\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('GBR_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because of the different boosting process, GBT models tend to work well even with weak base learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare AdaBoost and various GBT models on the photometric redshifts problem in the next notebook (FlavorsOfBoosting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
