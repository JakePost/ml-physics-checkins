{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we calculate the feature importance for a few tree-based ensemble methods, for the problem of predicting photometric redshifts from six photometric bands (u, g, r, i, z, y).\n",
    "\n",
    "It accompanies Chapter 6 of the book.\n",
    "\n",
    "Author: Viviana Acquaviva\n",
    "\n",
    "License: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "#matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = pd.read_csv('../data/sel_features.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_target = pd.read_csv('../data/sel_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_target.values.ravel() #changes shape to 1d row-like array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_features=4, n_estimators=200) #I need to re-seed the random state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been fit, it will have the attribute \"feature\\_importances\\_\". We can look at the feature importance using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(sel_features, sel_target.values.ravel()) \n",
    "\n",
    "#note: this is not doing any train/test split, but fitting the entire data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(sel_features.shape[1]):\n",
    "    print(\"%d. feature: %s, %d (%f)\" % (f + 1, sel_features.columns[indices[f]], indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(sel_features.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(sel_features.shape[1]), sel_features.columns[indices])\n",
    "plt.xlim([-1, sel_features.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this problem, actually all the features are quite important, but it's hard to diagnose issues. \n",
    "\n",
    "### Something we can do is to compare with the results of other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of three models\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.title(\"Feature importances for various models\")\n",
    "\n",
    "models = [RandomForestRegressor(max_features=4, n_estimators=200), \\\n",
    "          AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None), n_estimators=100), \n",
    "          xgb.XGBRegressor(objective ='reg:squarederror', max_depth=6, n_estimators = 500, learning_rate=0.1)]\n",
    "\n",
    "model_names = ['Random Forests', 'AdaBoost', 'XGBoost']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "\n",
    "    model.fit(sel_features, sel_target.values.ravel()) \n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.bar(np.arange(sel_features.shape[1])+0.1*i, importances, \n",
    "            align=\"center\", width=0.7, alpha = 0.5, label = model_names[i])\n",
    "    \n",
    "    plt.xticks(range(sel_features.shape[1]), sel_features.columns)\n",
    "    \n",
    "    plt.xlim([-1, sel_features.shape[1]])\n",
    "    \n",
    "    plt.legend(fontsize = 12)   \n",
    "    \n",
    "#    print('For model', model_names[i], 'features importances are', sel_features.columns[indices].values, importances[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reminder that feature importance is an indication only, and it is often algorithm-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The issue of leakage of test labels - recap\n",
    "\n",
    "- When we optimize parameters with a grid search, we choose the parameters that give the best test scores. This is different from what would happen with new data - to do this fairly, at no point of the training procedure we are allowed to look at the test labels. Therefore, one needs to do <b> nested cross validation </b> to evaluate the generalization error in order to avoid leakage between the parameter optimization and the cross validation procedure.\n",
    "<br>\n",
    "\n",
    "- Technically, standardizing/normalizing data using the entire learning set introduces leakage between train and test set (the test set \"knows\" about the mean and standard deviation of the entire data set). Usually not dramatic, but the correct procedure is to do it within each CV fold (i.e. after separating in train and test), only on the train set, and applying the same transformation to the test set. The model then becomes a pipeline.\n",
    "<br>\n",
    "\n",
    "- Technically, doing feature selection using the entire learning set introduces leakage between train and test set (the model \"picks\" features that give the best results on the test set). A possible solution is to pick the \"average\" best features within a cross-validated model.  \n",
    "<br>\n",
    "\n",
    "- In alternative, one can use unsupervised methods, for example by picking features with the largest variance; this is ok to do on the entire learning set, because it doesn't involve labels, but does not select features that are relevant for a specific supervised problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
