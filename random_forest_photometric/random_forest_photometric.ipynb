{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests and Photo-zs\n",
    "\n",
    "In this notebook, we use Random Forests to estimate photometric redshifts starting from observations of galaxy magnitudes in six different photometric bands (u, g, r, i, z, y). It accompanies Chapter 6 of the book.\n",
    "\n",
    "Essentially, we try to reproduce/improve upon the results of [this paper](https://arxiv.org/abs/1903.08174), for which the data are public and available [here](http://d-scholarship.pitt.edu/36064/).\n",
    "\n",
    "Author: Viviana Acquaviva, with contributions by Jake Postiglione and Olga Privman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "#fits stands for Flexible Image Transport System; it's a format that allows one to store images and summary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easiest IMO to read the data into a data frame using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open('../data/DEEP2_uniq_Terapix_Subaru_v1.fits') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder()) #see https://numpy.org/devdocs/user/basics.byteswapping.html#changing-byte-ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can select the columns I want, corresponding to the brightness of the galaxies in the six bands of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target property is the redshift. For this catalog, spectroscopic (more precise) redshifts are available in the column below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['zhelio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, we are now ready to run our first Random Forest model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what we are shooting for, we can look at the figure of the paper.\n",
    "\n",
    " ![Performance of photometric redshift reconstruction](Photoz_RF_CFHTLS_Deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, $\\sigma_{NMAD}$ is the normalized median absolute deviation of the residual vector, and $\\eta$  is the fraction of outliers, defined as those objects for which (z_true - z_est)/(1+z_true) > 0.15.\n",
    "\n",
    "To be fair, we are working with DEEP2/3 data, so our range is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(model,features,target, cv = KFold(n_splits=5, shuffle=True, random_state=10), return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - it takes a litle time! Also, the scores are the R2 score at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores['train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "What issue do these scores indicate?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "It looks like we have a severe high variance issue.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Why?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "We can see a large discrepancy between the train and the test scores (to be fair, we should also look at the standard deviation of the train and test scores to confirm that the gap is statistically significant).\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = cross_val_predict(model,features,target, cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(target,ypred, s = 20, c = 'royalblue')\n",
    "plt.xlabel('True (spectroscopic) z', fontsize=14)\n",
    "plt.ylabel('Predicted z',fontsize=14)\n",
    "plt.axis('square')\n",
    "plt.xlim(0,3)\n",
    "plt.ylim(0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Does it look like the one of the paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also interesting to look at the distribution of the predicted values, to see how they always tend to produce a narrower distribution. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(target,bins=50,density=False,alpha=0.5, range = (0,3), label = 'True');\n",
    "plt.hist(ypred,bins=50,density=False,alpha=0.5, range = (0,3), color = 'g', label = 'Predicted');\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we are now ready to calculate the outlier fraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.abs(target-ypred)>0.15*(1+target))[0])/len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And NMAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.48*np.median(np.abs(target-ypred)/(1 + target)) \n",
    "#The 1.48 is there because for a Gaussian distribution, this becomes the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a pretty severe high variance issue, so we can perform some parameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by making the data set a bit smaller, as we have seen that timings were already challenging in simple k-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "sel = np.random.choice(range(len(ypred)), 5000, replace = False) #sample without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(sel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create our new smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seld = features.loc[sel,:]\n",
    "selt = target[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "littlescores = cross_validate(model,seld,selt, cv = KFold(n_splits=5, shuffle=True, random_state=10), return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "littlescores['test_score'].mean(), littlescores['train_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar performance on training set ensures that size of data set is not a big issue and we can proceed with optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Parameters\n",
    "\n",
    "The parameters associated to that are:\n",
    "\n",
    "-  The minimum number of instances in a leaf node;\n",
    "\n",
    "-  The minimum number of instances required in a split node;\n",
    "\n",
    "- The maximum depth of tree.\n",
    "\n",
    "-  The criterion chosen to decide whether a split is \"worth it\", expressed in terms of information gain;\n",
    "\n",
    "\n",
    "#### Randomization Parameters\n",
    "\n",
    "- The number of k < n features that are used in building trees.\n",
    "\n",
    "- The re-sampling (boostrap) of the data set (T or F).\n",
    "\n",
    "\n",
    "#### Forest Parameters\n",
    "\n",
    "The number of trees in the forest (n_estimators) can be adjusted, with the general understanding that more trees are better, but at some point performance will plateau, so one can find the trade-off between having more trees and lower runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss a possible set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- min_impurity_decrease \n",
    "\n",
    "- number of trees\n",
    "\n",
    "- max_leaf_nodes\n",
    "\n",
    "- min_samples_split\n",
    "\n",
    "- max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a few minutes\n",
    "\n",
    "parameters = {'min_impurity_decrease':[0.1, 0.5, 0.0], \\\n",
    "              'max_features':[None,4,2], 'n_estimators':[50, 100, 200], 'min_samples_split': [10,20,100], \n",
    "              'max_leaf_nodes':[None, 100, 200]}\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(RandomForestRegressor(), parameters, cv = KFold(n_splits=5, shuffle=True), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(seld,selt)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the verdict is...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have NOT improved the test scores.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to look at data cleaning and/or imputing!\n",
    "\n",
    "In my case, it was time to write to the authors of the paper, who told me exactly how they selected the data that went into making the learning set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor','subaru_source','cfhtls_source','zquality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redshift quality - only use objects with high-quality spectroscopic redshift measurements\n",
    "\n",
    "mags = mags[mags['zquality'] >= 3]\n",
    "\n",
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfhtls deep photo \n",
    "\n",
    "mags = mags[mags['cfhtls_source'] == 0]\n",
    "\n",
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's leave this out for now (subaru deep photo)\n",
    "\n",
    "#mags = mags[mags['subaru_source'] == 0]\n",
    "\n",
    "#mags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unavailable measurements are marked by -99 or 99 (while typical values are around 20-25). We can get rid of data with missing measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags > -10].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags < 90].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final set is made of the six original features and it has 6,307 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = mags[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']]\n",
    "sel_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need, of course, to select the same set on the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_target = target[sel_features.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our benchmark model works. Note that for reproducible results we need to fix the random\\_state parameter of the Random Forest (which controls the bootstrap process) and the random seed of the cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(RandomForestRegressor(random_state = 5),sel_features,sel_target,cv = KFold(n_splits=5, shuffle=True, random_state=10), \\\n",
    "               return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(np.mean(scores['test_score']),3), np.round(np.std(scores['test_score']),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(np.mean(scores['train_score']),3), np.round(np.std(scores['train_score']),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores have improved noticeably! However, we can still observe high variance. We can re-run the optimization process (note that the data set size is limited, so we don't need to make it smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This now takes ~3 minutes\n",
    "\n",
    "parameters = {'max_depth':[3, 6, None], \\\n",
    "              'max_features':[None,4,2], 'n_estimators':[50,100,200], 'min_samples_leaf': [1,5,10]}\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(RandomForestRegressor(random_state = 5), parameters, cv = KFold(n_splits=5, shuffle=True, random_state=10), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(sel_features,sel_target)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n",
    "scoresCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Based on the results from the Grid Search, would you expect that a significant improvement come come from enlarging the space of parameters?\n",
    "\n",
    "<br>\n",
    "<details><summary><b>Click here for the answer!</b></summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "Probably not, because the scores are flat-ish over the first 10-20 models, suggesting that further optimization is unlikely to help significantly.\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate one set of predictions to visualize what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = cross_val_predict(bm, sel_features,sel_target, cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(sel_target,ypred, s =10)\n",
    "plt.xlabel('z_spec')\n",
    "plt.ylabel('z_photo')\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate outlier fraction and compare with the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.abs(sel_target-ypred)>0.15*(1+sel_target))[0])/len(sel_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Normalized Median Absolute Deviation (NMAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1.48*np.median(np.abs(sel_target-ypred)/(1 + sel_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: How does our model compare with the paper's results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing slightly worse, but we have a secret weapon in feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering exercise: what happens if I use colors instead of magnitudes? (below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features.loc[:,'u-g'] = sel_features['u_apercor'] - sel_features['g_apercor']\n",
    "sel_features.loc[:,'g-r'] = sel_features['g_apercor'] - sel_features['r_apercor']\n",
    "sel_features.loc[:,'r-i'] = sel_features['r_apercor'] - sel_features['i_apercor']\n",
    "sel_features.loc[:,'i-z'] = sel_features['i_apercor'] - sel_features['z_apercor']\n",
    "sel_features.loc[:,'z-y'] = sel_features['z_apercor'] - sel_features['y_apercor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_colors = sel_features[['u-g','g-r','r-i','i-z','z-y','i_apercor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(RandomForestRegressor(),sel_colors,sel_target,cv = KFold(n_splits=5, shuffle=True, random_state=10), \\\n",
    "               return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['test_score'].mean(), scores['test_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[3, 6, None], \\\n",
    "              'max_features':[None,4,2], 'n_estimators':[50,100,200], 'min_samples_leaf': [1,5,10]}\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(RandomForestRegressor(), parameters, cv = KFold(n_splits=5, shuffle=True), \\\n",
    "                     verbose = 2, n_jobs = 4, return_train_score=True)\n",
    "model.fit(sel_colors, sel_target)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score','mean_fit_time']].sort_values(by = 'mean_test_score', \\\n",
    "                                                    ascending = False)\n",
    "scoresCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = cross_val_predict(bm, sel_colors, sel_target, cv = KFold(n_splits=5, shuffle=True, random_state=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Calculate the Normalized Median Absolute Deviation (NMAD) and the outlier fraction of the predicted redshifts with respect to the true redshifts by filling in the code below.\n",
    "\n",
    "```python\n",
    "1.48 * np.median(... (... - ...)/(1 + ...))\n",
    "\n",
    "len(... (np.abs(...) > ... * (1 + ...))[0]) / len(...)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "1.48 * np.median(np.abs(sel_target-ypred)/(1 + sel_target))\n",
    "\n",
    "len(np.where(np.abs(sel_target-ypred)>0.15*(1+sel_target))[0])/len(sel_target)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(sel_target,ypred, s =10)\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally matched the performance of the paper (although, to be fair, we are not using *exactly* the same set of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Until now, we have never discussed a very important point: how to estimate the uncertainty associated to our results. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One source of scatter on the global performance metrics comes from the system's architecture: we should generate a bunch of predictions with several random seeds. This is equivalent to the scatter found in the monitored metric (e.g., MSE or r2 score) in cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_features=4, n_estimators=200) #I need to re-seed the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: this also takes time!\n",
    "\n",
    "seeds = np.random.choice(100,8, replace = False) #pick 8\n",
    "\n",
    "olf = np.zeros(8)\n",
    "NMAD = np.zeros(8)\n",
    "\n",
    "for i in range(8): #A bit rough, but it gives a sense of what happens by varying the random seeds!\n",
    "    print('Iteration', i)\n",
    "    ypred = cross_val_predict(RandomForestRegressor(max_features=4, n_estimators=200,random_state=seeds[i]), sel_features, sel_target, cv = KFold(n_splits=5, shuffle=True, random_state=seeds[i]))\n",
    "    olf[i] = len(np.where(np.abs(sel_target-ypred)>0.15*(1+sel_target))[0])/len(sel_target)\n",
    "    NMAD[i] = 1.48*np.median(np.abs(sel_target-ypred)/(1 + sel_target))\n",
    "\n",
    "print('OLF avg/std:, {0:.5f}, {1:0.5f}'.format(olf.mean(), olf.std()))\n",
    "print('NMAD avg/std:, {0:.5f}, {1:0.5f}'.format(NMAD.mean(), NMAD.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, we should also think about how to quantify the observational error on our *individual* inputs.\n",
    "\n",
    "The literature on this subjects is scarce, but my proposal is to run a forward-pass of the best model a bunch of times, with different inputs that are drawn from modeling each input according to their noise profile (e.g. a Gaussian where mean = observed value and sigma = experimental error).\n",
    "\n",
    "Assuming that the noise profile doesn't change from the \"true\" value to the \"observed\" value, this should include the experimental error and the \"limited information\" error, which comes from limited training set size, lack of informative features, model architecture etc. This has been described as epistemic vs aleatoric uncertainty (see [this recent review](https://link.springer.com/article/10.1007/s10994-021-05946-3))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
