{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple notebook to show the effect of Ridge and Lasso regularization.\n",
    "\n",
    "It accompanies Chapter 5 of the book (3 of 5).\n",
    "\n",
    "Author: Viviana Acquaviva, with contributions by Jake Postiglione and Olga Privman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16) #set seed for reproducibility purposes\n",
    "\n",
    "x1 = np.arange(100) \n",
    "\n",
    "x2 = np.linspace(0,1,100)\n",
    "\n",
    "x3 = np.logspace(2,3,num=100) \n",
    "\n",
    "ypb = 3*x1 + 0.5*x2 + 15*x3 + 3 + 5*(np.random.poisson(3*x1 + 0.5*x2 + 15*x3,100)-(3*x1 + 0.5*x2 + 15*x3)) \n",
    "                                                    #generate some data with scatter following Poisson distribution \n",
    "                                                    #with exp value = y from linear model, centered around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.vstack((x1,x2,x3)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add correlated features (polynomial transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xb = poly.fit_transform(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "How many features will the transformed data set have? You can think of it first, and then use code to find out.\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "new_xb.shape\n",
    "\n",
    "will show that there are 9 features (the original ones, plus all the monomial combinations of x1, x2, and x3 up to degree 1, for example x1^2, x1 x2...)\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with Ridge regression, and tune alpha using cross-validation.\n",
    "\n",
    "(note what happens if repeating a few times without fixing the random seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = []\n",
    "\n",
    "for alpha in np.logspace(-6,6,13):\n",
    "\n",
    "    model_reg = Ridge(alpha = alpha, normalize = True) #normalization helps\n",
    "\n",
    "    scores = cross_validate(model_reg, new_xb, ypb, cv = KFold(n_splits=10, shuffle=True, random_state = 1), scoring = 'neg_mean_squared_error')\n",
    "\n",
    "    print(alpha, np.round(-np.mean(scores['test_score'])))\n",
    "    \n",
    "    MSE.append(-np.mean(scores['test_score']))\n",
    "\n",
    "print('Best alpha:', np.logspace(-6,6,13)[np.argmin(MSE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is also a built-in instrument for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regm = RidgeCV(alphas=np.logspace(-6,6,13), normalize=True, cv = KFold(n_splits=10, shuffle=True, random_state=1),\\\n",
    "             scoring = 'neg_mean_squared_error')\n",
    "\n",
    "regm.fit(new_xb,ypb) #this calls the \"fit\" method, which means that I obtain the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can compare the coefficients of the linear model for different amounts of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's pick alpha = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha = 1000., normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(new_xb,ypb)\n",
    "\n",
    "coef_alpha_1000 =  np.hstack([model.coef_, model.intercept_]) #Note the intercept is the last number\n",
    "\n",
    "print(coef_alpha_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see for alpha = 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Will the coefficients be larger or smaller, compared to the case with alpha = 1000?\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "The coefficients will be larger, because we use a weaker regularization, and the effect of regularization in a linear model is to keep the coefficients small.\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha = 1., normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(new_xb,ypb)\n",
    "\n",
    "coef_alpha_1 =  np.hstack([model.coef_, model.intercept_]) #Note the intercept is the last number\n",
    "\n",
    "print(coef_alpha_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, we use a trick to get coefficients for \"zero\" alpha (no regularization); I could have also used LinearRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha = 1e-7, normalize = True)\n",
    "\n",
    "model.fit(new_xb,ypb)\n",
    "\n",
    "coef_no_reg =  np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "print(coef_no_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can compare the coefficients for the three cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.bar(np.arange(10)-0.2, np.abs(coef_alpha_1000), color = 'maroon',width=0.05, label = 'Ridge, alpha = 1000')\n",
    "plt.bar(np.arange(10)-0.1, np.abs(coef_alpha_1), color = 'orangered',width=0.05, label = 'Ridge, alpha = 1.0')\n",
    "plt.bar(range(10), np.abs(coef_no_reg), color = 'grey',width=0.05, label = 'Linear (no regularization)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xticks(np.arange(10), ['1','2', '3','4','5','6','7','8','9', 'Intercept'])  # Set text labels.\n",
    "\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "\n",
    "plt.ylabel('Coefficients (absolute value)',fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: LassoCV re-orders alphas in DESCENDING ORDER! Scores will be messed up unless you use model.alphas_ object\n",
    "\n",
    "model = LassoCV(alphas = np.logspace(-2,2,5), cv = KFold(n_splits=10, shuffle=True, random_state=1), \\\n",
    "              max_iter = 1000000, tol = 1e-6, normalize=True)\n",
    "\n",
    "model.fit(new_xb,ypb)\n",
    "\n",
    "print('Alphas', model.alphas_)\n",
    "\n",
    "print('Best alpha:', model.alpha_)\n",
    "\n",
    "for i, alpha in enumerate(model.alphas_):\n",
    "    print('Score for alpha', alpha, np.mean(model.mse_path_[i,:])) #for each alpha (row), 10 cv estimates of MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'red'> Note: early reproducibility issues were solved by setting tolerance to a small value (thanks to Joel Zinn!). </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can of course also use the \"regular\" Lasso and do CV ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the coefficients for alpha = 1000 and alpha = 1. Lasso regularization tends to induce sparse coefficients, so we can check that that's true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1000 = Lasso(alpha = 1000, max_iter = 1000000, tol = 0.005, normalize=True)\n",
    "\n",
    "L1000.fit(new_xb, ypb)\n",
    "\n",
    "coef_L1000 =  np.hstack([L1000.coef_, L1000.intercept_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_L1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Check-in\n",
    "    \n",
    "Should we be worried because \n",
    "\n",
    "1) The intercept has become very large?\n",
    "\n",
    "2) All the coefficients have disappeared?\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary style=\"display: list-item;\">Click here for the answer!</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "1) Not really, because the intercept is excluded from the regularization process;\n",
    "\n",
    "2) Only if this happens when using the same code, and a much smaller value for alpha :) \n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = Lasso(alpha = 1.0, max_iter = 1000000, tol = 0.005, normalize=True)\n",
    "\n",
    "L1.fit(new_xb, ypb)\n",
    "\n",
    "coef_L1 = np.hstack([L1.coef_, L1.intercept_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_no_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we can plot all the coefficients together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.bar(np.arange(10)-0.2, np.abs(coef_alpha_1000), color = 'maroon',width=0.05, label = 'Ridge, alpha = 1000')\n",
    "plt.bar(np.arange(10)-0.1, np.abs(coef_alpha_1), color = 'orangered',width=0.05, label = 'Ridge, alpha = 1.0')\n",
    "plt.bar(range(10), np.abs(coef_no_reg), color = 'grey',width=0.05, label = 'Linear (no regularization)')\n",
    "plt.bar(np.arange(10)+0.1, np.abs(coef_L1), color = 'tab:cyan',width=0.05, label = 'Lasso, alpha = 1.0')\n",
    "plt.bar(np.arange(10)+0.2, np.abs(coef_L1000), color = 'tab:blue', width=0.05, label = 'Lasso, alpha = 1000')\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xticks(np.arange(10), ['1','2', '3','4','5','6','7','8','9', 'Intercept'])  # Set text labels.\n",
    "\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "\n",
    "plt.ylabel('Coefficients (absolute value)',fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=13, bbox_to_anchor=(1.05, 1));\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
